{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forty-label",
   "metadata": {},
   "source": [
    "# PSI Numerical Methods 2023, homework assignment for MCMC Week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-dependence",
   "metadata": {},
   "source": [
    "For this assignment, we will look at the efficiency (convergence speed) for four different MCMC approaches:\n",
    "* plain old Metropolis-Hastings with (uncorrelated) Gaussian jumps, and poorly chosen jump sizes\n",
    "* Metropolis-Hastings with (uncorrelated) Gaussian jumps, and well chosen jump sizes\n",
    "* Metropolis-Hastings with correlated Gaussian jumps\n",
    "* the Affine-invariant ensemble sampler\n",
    "\n",
    "\n",
    "\n",
    "When you are ready to check in your code, please\n",
    "* make sure to click the \"save\" button on the notebook\n",
    "* In Jupyterhub, open a `Terminal` session\n",
    "* `cd PSI-MCMC-project`\n",
    "* `git commit -a -m \"update notebook\"`\n",
    "* `git push`\n",
    "\n",
    "In this notebook, I will highlight your tasks in **bold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minimal-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ]add CairoMakie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "personalized-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CairoMakie\n",
    "using LinearAlgebra\n",
    "using AffineInvariantMCMC\n",
    "using Statistics\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-scout",
   "metadata": {},
   "source": [
    "We will start by defining the same data and functions that you're probably bored of by now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decimal-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data set from  arxiv:1008.4686, table 1 (https://arxiv.org/abs/1008.4686)\n",
    "# You can also refer to that paper for more background, equations, etc.\n",
    "alldata = [201. 592 61; 244 401 25; 47  583 38; 287 402 15; 203 495 21; 58  173 15; 210 479 27;\n",
    "           202 504 14; 198 510 30; 158 416 16; 165 393 14; 201 442 25; 157 317 52; 131 311 16;\n",
    "           166 400 34; 160 337 31; 186 423 42; 125 334 26; 218 533 16; 146 344 22 ]\n",
    "# The first 5 data points are outliers; for this we'll just use the \"good\" data points\n",
    "x    = alldata[6:end, 1]\n",
    "y    = alldata[6:end, 2]\n",
    "# this is the standard deviation (uncertainty) on the y measurements, also known as \\sigma_i\n",
    "yerr = alldata[6:end, 3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "agreed-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_likelihood_one(params, x, y, yerr)\n",
    "    \"\"\"This function computes the log-likelihood of a data set with coordinates\n",
    "    (x_i,y_i) and Gaussian uncertainties on y_i of yerr_i (aka sigma_i)\n",
    "\n",
    "    The model is a straight line, so the model's predicted y values are\n",
    "        y_pred_i = b + m x_i.\n",
    "\n",
    "    params = (b,m) are the parameters (scalars)\n",
    "    x,y,yerr are arrays (aka vectors)\n",
    "\n",
    "    Return value is a scalar log-likelihood.\n",
    "    \"\"\"\n",
    "    # unpack the parameters\n",
    "    b,m = params\n",
    "    # compute the vector y_pred, the model predictions for the y measurements\n",
    "    y_pred = b .+ m .* x\n",
    "    # compute the log-likelihoods for the individual data points\n",
    "    # (the quantity inside the sum in the text above)\n",
    "    loglikes = log.(1 ./ (sqrt(2*Ï€) .* yerr)) .- 0.5 .*(y - y_pred).^2 ./ yerr.^2\n",
    "    # the log-likelihood for the whole vector of measurements is the sum of individual log-likelihoods\n",
    "    loglike = sum(loglikes)\n",
    "    return loglike\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-alias",
   "metadata": {},
   "source": [
    "And here's the \"vanilla\" Metropolis-Hasting Markov Chain Monte Carlo from the MCMC-filled-in notebook.  Please feel free to use your own if you prefer it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "revised-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "function mcmc(logprob_func, logprob_args,\n",
    "              propose_func, propose_args,\n",
    "              initial_pos, nsteps)\n",
    "    \"\"\"\n",
    "    MCMC: Markov Chain Monte Carlo.  Draw samples from the *logprob_func* probability distribution,\n",
    "    using proposed moves generated by the function *propose_func*.\n",
    "\n",
    "    * logprob_func: a function that returns the log-probability at a given value of parameters.\n",
    "               It will get called like this:\n",
    "        lnp = logprob_func(params, logprob_args)\n",
    "    * logprob_args: extra arguments to pass to logprob_func.\n",
    "    * propose_func: a function that proposes to jump to a new point in parameter space.\n",
    "               It will get called like this:\n",
    "        p_new = propose_func(p, propose_args)\n",
    "    * propose_args: extra arguments to pass to propose_func.\n",
    "    * initial_pos: initial position in parameter space (list/array)\n",
    "    * nsteps: integer number of MCMC steps to take\n",
    "    \n",
    "    Returns  (chain, faccept)\n",
    "    * chain: size Nsteps x P, MCMC samples\n",
    "    * faccept: float: fraction of proposed jumps that were accepted\n",
    "    \"\"\"\n",
    "    p = initial_pos\n",
    "    logprob = logprob_func(p, logprob_args)\n",
    "    chain = zeros(Float64, (nsteps, length(p)))\n",
    "    naccept = 0\n",
    "    for i in 1:nsteps\n",
    "        # propose a new position in parameter space\n",
    "        p_new = propose_func(p, propose_args)\n",
    "        # compute probability at new position\n",
    "        logprob_new = logprob_func(p_new, logprob_args)\n",
    "        # decide whether to jump to the new position\n",
    "        if exp(logprob_new - logprob) > rand()\n",
    "            p = p_new\n",
    "            logprob = logprob_new\n",
    "            naccept += 1\n",
    "        end\n",
    "        # save the position\n",
    "        chain[i,:] = p\n",
    "    end\n",
    "    return chain, naccept/nsteps\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-avenue",
   "metadata": {},
   "source": [
    "Here is the proposal distribution function for the simple (uncorrelated) Gaussian.  The samples from this function will lie in an axis-aligned ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seven-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "function propose_gaussian(p, stdevs)\n",
    "    \"\"\"\n",
    "    A Gaussian proposal distribution for mcmc.\n",
    "    *p*: the point in parameter space to jump from\n",
    "    *stdevs*: standard deviations for each dimension in the parameter space.\n",
    "    \"\"\"\n",
    "    return p .+ randn(length(p)) .* stdevs\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-horror",
   "metadata": {},
   "source": [
    "I'm asking you to act like Bayesians, so instead of the log-likelihood we're going to use the log-posterior (but with a flat prior...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "established-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_posterior_one(params, args)\n",
    "    (x, y, yerr) = args\n",
    "    loglike = log_likelihood_one(params, x, y, yerr)\n",
    "    # Improper, flat priors on params!\n",
    "    logprior = 0.\n",
    "    return loglike + logprior\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-jordan",
   "metadata": {},
   "source": [
    "## Part 1.  Metropolis-Hastings MCMC, with uncorrelated Gaussian jumps and bad jump sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "western-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of moves accepted:0.3602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial B,M\n",
    "initial_pos = [0., 1.0]\n",
    "# proposal distribution: jump sizes for B,M\n",
    "jump_sizes = [1., 0.1]\n",
    "\n",
    "# How many MCMC steps to take\n",
    "nsteps = 5000\n",
    "# How many samples to throw away as \"burn-in\"\n",
    "nburn = 1000\n",
    "\n",
    "# Run MCMC!\n",
    "wholechain,accept = mcmc(log_posterior_one, (x,y,yerr),\n",
    "                         propose_gaussian, jump_sizes,\n",
    "                         initial_pos, nsteps)\n",
    "println(\"Fraction of moves accepted:\", accept)\n",
    "chain = wholechain[nburn+1:end, :]\n",
    "size(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-editing",
   "metadata": {},
   "source": [
    "Now, let's have a look at the convergence.  Looking at the samples for just one of `B` or `M`,\n",
    "let's compute the mean and standard deviation of all the samples up to different lengths of the\n",
    "chain.  (Eg, the mean and standard deviation after 1000 steps, then 2000 steps, then 3000 steps.)\n",
    "If our chain is converging, we should see that the mean and variance do not change much as we take\n",
    "more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "periodic-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the mean and standard deviation of array `x` up to index `steps`, `2*steps`, etc.\n",
    "function running_mean_std(x, steps)\n",
    "    n = size(x,1)\n",
    "    # the list of steps that we're going to measure the mean and std up to.\n",
    "    ss = steps:steps:n\n",
    "    mn = zeros(size(ss,1))\n",
    "    st = zeros(size(ss,1))\n",
    "    for i in 1:size(ss,1)\n",
    "        mn[i] = mean(x[1:ss[i]])\n",
    "        st[i] =  std(x[1:ss[i]])\n",
    "    end\n",
    "    return ss,mn,st\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-departure",
   "metadata": {},
   "source": [
    "**Does this chain look like it has converged?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-clothing",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "No the mean and standard deviation do not look like they have converged to a specific value and did not level out to a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-penguin",
   "metadata": {},
   "source": [
    "One way we would use the samples from our MCMC is to report the values, say in a paper we are writing.\n",
    "\n",
    "**Looking at the numbers in the `mn` and `st` arrays from above (the mean and standard deviation of the first 500 samples, first 1000 samples, first 1500, etc), please write out what you would report for the mean and uncertainty of `M`, in the form $X \\pm Y$.  For example, to print them out for the first 500 samples, you could use the code below.  Please write out those values for all the entries in the `mn` and `st` arrays.  If you were writing a paper, you would just use the last entry.  Are you happy with that value, or do you think that if you ran the chain for longer, it would settle on a different final value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-shipping",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "I am not sure that if we ran the chain for longer it would converge to a better value, given that the errors grew beyond what they were initially they have not had the time to settle to a better value. I think tweaking the jump sizes might allow it to converge to a stable point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-chile",
   "metadata": {},
   "source": [
    "## Part 2.  Metropolis-Hastings MCMC, with uncorrelated Gaussian jumps and good jump sizes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-chaos",
   "metadata": {},
   "source": [
    "Please repeat the steps above, but with `jump_sizes` that you found in the tutorial session that resulted in good acceptance fractions.  (If you need to, you can also check later on in the `MCMC-filled-in` notebook!)\n",
    "\n",
    "Here we substitute the jump sizes found in `MCMC-filled-in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial B,M\n",
    "initial_pos = [0., 1.0]\n",
    "# proposal distribution: jump sizes for B,M\n",
    "jump_sizes = [10., 0.05]\n",
    "\n",
    "# How many MCMC steps to take\n",
    "nsteps = 5000\n",
    "# How many samples to throw away as \"burn-in\"\n",
    "nburn = 1000\n",
    "\n",
    "# Run MCMC!\n",
    "wholechain,accept = mcmc(log_posterior_one, (x,y,yerr),\n",
    "                         propose_gaussian, jump_sizes,\n",
    "                         initial_pos, nsteps)\n",
    "println(\"Fraction of moves accepted:\", accept)\n",
    "chain = wholechain[nburn+1:end, :]\n",
    "size(chain)\n",
    "\n",
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "\n",
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-virus",
   "metadata": {},
   "source": [
    "**With better step sizes, what do you observe about how much the running mean and standard deviation changes?  Does it look like the chain has converged after 5000 steps?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-pride",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Yes it looks much more stable and the values do not change much as the steps continue. The standard deviation is still large though I would expect a better convergence to reduce the error bars more but I could not find a better jump size to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-header",
   "metadata": {},
   "source": [
    "## Part 3.  Metropolis-Hastings MCMC, with correlated Gaussian jumps and good jump sizes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-triangle",
   "metadata": {},
   "source": [
    "In the first two parts, we are using the `propose_gaussian` function to propose the jumps to take.  That function takes an array of two standard deviations, one for the `B` direction and one for the `M` direction.  It draws two random Gaussian values and multiplies one by the `B` step size and the other by the `M` step size.  This results in a proposal ellipse that is axis-aligned in the `B`, `M` plane.\n",
    "\n",
    "In this part, I want you to write a proposal function that proposes jumps in the `B,M` plane that are Gaussian distributed but with a covariance between the `B` and `M` values, so that the proposals are inclined in the `B,M` plane, approximately matching the shape of the likelihood contours.\n",
    "\n",
    "We are going to *cheat* a bit by using the `chain` computed above to define our proposal distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take the \"chain\" computed above and compute its covariance using the cov() function.\n",
    "C = cov(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-perth",
   "metadata": {},
   "source": [
    "This is the covariance matrix between `M` and `B`.  The diagonals are the variances of the individual variables -- we know that `B` has a larger spread of allowed values, so its variance is larger than the variance of `M`.  The covariance term, which is symmetric, is negative -- the variables are *anti-*correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-manor",
   "metadata": {},
   "source": [
    "**Please complete the `propose_gaussian_cov` function below, and confirm that it works by drawing 1000 samples centered on point `(0,0)` and confirming that the covariance is approximately equal to the desired covariance (C), and plot the samples and confirm that they are strongly anti-correlated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-settle",
   "metadata": {},
   "source": [
    "By googling the suggested string I found that you can use the spectral decomposition to sample a vector from a multivariate normal distrobution. We get the new vector by \n",
    "\n",
    "$$cov = U * Diagonal(S) * Vt$$\n",
    "\n",
    "$$X = p + U * Diagonal(\\sqrt{S}) * g_sample$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "function propose_gaussian_cov(p, cov)\n",
    "    \"\"\"\n",
    "    A Gaussian proposal distribution for mcmc, that draws samples with covariance = *cov*.\n",
    "    \n",
    "    That is, it draws samples from the multivariate Gaussian.\n",
    "    \n",
    "    *p*: the point in parameter space to jump from\n",
    "    *cov*: the covariance between the points\n",
    "    \"\"\"\n",
    "\n",
    "    # Hint: You will probably want to draw two independent regular Gaussian samples and then matrix-multiply them by something!\n",
    "    g_sample = randn(length(p))\n",
    "\n",
    "    ### FILL ME IN!\n",
    "    SVD = svd(cov)\n",
    "    A = SVD.U * Diagonal(sqrt.(SVD.S)) * g_sample\n",
    "\n",
    "    \n",
    "    return p .+ A\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-ministry",
   "metadata": {},
   "source": [
    "(Hint: \"sample from gaussian covariance\" is a pretty good Google search phrase!  Another hint: you have the *covariance*\n",
    "but you want to multiply your independent Gaussian samples by something that is like the *matrix square root* of the covariance!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test your `propose_gaussian_cov` function, you can try drawing 1,000 samples from it, centered on (0,0) :\n",
    "pp = Float64[]\n",
    "for i in 1:1000\n",
    "    append!(pp, propose_gaussian_cov([0.,0.], C))\n",
    "end\n",
    "pp = reshape(pp, 2,:)';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-month",
   "metadata": {},
   "source": [
    "**Compute the covariance of the samples generated by your function.  Do they match the desired covariance, C?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov(pp) .- C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-duplicate",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Yes this matrix is very close to what we expected from our desired covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-cleanup",
   "metadata": {},
   "source": [
    "**Plot the samples generated by your function.  Are they an inclined ellipse with the expected larger scatter in B than in M?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Figure()\n",
    "Axis(f[1, 1], xlabel=\"B\", ylabel=\"M\", title=\"MCMC Samples\")\n",
    "plot!(pp[:,1], pp[:,2])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-tiffany",
   "metadata": {},
   "source": [
    "FINALLY, let's run the MCMC algorithm using your new proposal function (and the target covariance matrix)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial B,M\n",
    "initial_pos = [0., 1.0]\n",
    "# Run MCMC!\n",
    "chain,accept = mcmc(log_posterior_one, (x,y,yerr),\n",
    "                    propose_gaussian_cov, C,\n",
    "                    initial_pos, nsteps)\n",
    "println(\"Fraction of moves accepted:\", accept)\n",
    "chain = chain[nburn+1:end, :]\n",
    "size(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-survival",
   "metadata": {},
   "source": [
    "**Repeat the plots we made in Part 1 and compare the results.  Does it look like this chain is converged after 5,000 steps?  Did the burn-in take the same amount of steps, or was it maybe slower or faster?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "\n",
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-harrison",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Wow that is a very stable mean and standard deviation after each 500 steps. Very good! It looks to have converged even before the burn-in finished. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-maria",
   "metadata": {},
   "source": [
    "## Part 4.  The Affine-invariant sampler.\n",
    "\n",
    "In this part, we'll use the affine-invariant ensemble sampler.\n",
    "\n",
    "Since the ensemble sampler moves a *group* of *walkers* at once, we have to be a bit careful how we compare it to regular MCMC.  Probably the fairest comparison is if you make the total number of samples the same.  That is, run the ensemble sampler for `n_ensemble = n_mcmc / n_walkers` where `n_mcmc` was the number of steps you took in the regular MCMC chain.\n",
    "\n",
    "You may see, however, that the burn-in takes longer.  So you might want to either initialize the ensemble closer to the correct answer, or run for more burn-in steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the sampler a number of things:\n",
    "# - how many dimensions are being sampled\n",
    "numdims = 2\n",
    "# - how many walkers we want\n",
    "numwalkers = 25\n",
    "# In the example code below, it runs an initial \"burn-in\" round of sampling, then does the \"real\" sampling.  This is how many\n",
    "# burn-in samples to take.\n",
    "nburn_aff = nburn Ã· numwalkers\n",
    "# - how many steps to take (after burn-in)\n",
    "nsteps_aff = (nsteps - nburn) Ã· numwalkers\n",
    "\n",
    "# We also need to give the walkers some initial positions.  NOTE that you can't give them all the same position!\n",
    "# Here, we're just drawing uniform numbers of between 0 and 1.\n",
    "x0 = rand(numdims, numwalkers)\n",
    "### FILL ME IN -- maybe you want to initialize the walkers around some better point?\n",
    "# x0[:,1] += ...?\n",
    "# x0[:,2] += ...?\n",
    "\n",
    "# We need to pass to the sampler a function that takes *only* the vector of parameters.  Our log_posterior_one function\n",
    "# also needs the (x,y,yerr) values.  So to make this work, we need a \"wrapper\" function to can grab the (x,y,yerr) values\n",
    "# and pass them to log_likelihood_one.\n",
    "\n",
    "ll_func(bm) = log_posterior_one(bm, (x, y, yerr))\n",
    "\n",
    "# Here we go, let's call the AffineInvariant MCMC's \"sample\" function -- this is the burn-in round!\n",
    "chain, _ = AffineInvariantMCMC.sample(ll_func, numwalkers, x0, nburn_aff, 1)\n",
    "\n",
    "# And here's the \"real\" run.\n",
    "# Notice that it's passing in the end of the burn-in chain as the initial position!\n",
    "chain, llvals = AffineInvariantMCMC.sample(ll_func, numwalkers, chain[:, :, end], nsteps_aff, 1)\n",
    "\n",
    "# And that's it!  Now \"chain\" contains our samples!\n",
    "println(\"Chain:\", size(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can \"flatten\" that list of samples so that it's a (N x 2) array like for regular MCMC.\n",
    "flatchain, flat_llvals = AffineInvariantMCMC.flattenmcmcarray(chain, llvals);\n",
    "chain = flatchain'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-franklin",
   "metadata": {},
   "source": [
    "**Now please repeat the plots and the $M = mean \\pm std$ printing from Part 1.  Does it look like the ensemble sampler is converging faster?  Does the burn-in look faster or slower?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "\n",
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-writing",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "It definetly took longer to converge and burn-in. Not the best result. Lets try with more walkers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-restoration",
   "metadata": {},
   "source": [
    "**Try changing the number of walkers.  Try 10, or 100.  How do the burn-in and convergence speed look?  Of 10, 20, or 100 walkers, which one looks best to you?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the sampler a number of things:\n",
    "# - how many dimensions are being sampled\n",
    "numdims = 2\n",
    "# - how many walkers we want\n",
    "numwalkers = 10\n",
    "# In the example code below, it runs an initial \"burn-in\" round of sampling, then does the \"real\" sampling.  This is how many\n",
    "# burn-in samples to take.\n",
    "nburn_aff = nburn Ã· numwalkers\n",
    "# - how many steps to take (after burn-in)\n",
    "nsteps_aff = (nsteps - nburn) Ã· numwalkers\n",
    "\n",
    "# We also need to give the walkers some initial positions.  NOTE that you can't give them all the same position!\n",
    "# Here, we're just drawing uniform numbers of between 0 and 1.\n",
    "x0 = rand(numdims, numwalkers)\n",
    "### FILL ME IN -- maybe you want to initialize the walkers around some better point?\n",
    "# x0[:,1] += ...?\n",
    "# x0[:,2] += ...?\n",
    "\n",
    "# We need to pass to the sampler a function that takes *only* the vector of parameters.  Our log_posterior_one function\n",
    "# also needs the (x,y,yerr) values.  So to make this work, we need a \"wrapper\" function to can grab the (x,y,yerr) values\n",
    "# and pass them to log_likelihood_one.\n",
    "\n",
    "ll_func(bm) = log_posterior_one(bm, (x, y, yerr))\n",
    "\n",
    "# Here we go, let's call the AffineInvariant MCMC's \"sample\" function -- this is the burn-in round!\n",
    "chain, _ = AffineInvariantMCMC.sample(ll_func, numwalkers, x0, nburn_aff, 1)\n",
    "\n",
    "# And here's the \"real\" run.\n",
    "# Notice that it's passing in the end of the burn-in chain as the initial position!\n",
    "chain, llvals = AffineInvariantMCMC.sample(ll_func, numwalkers, chain[:, :, end], nsteps_aff, 1)\n",
    "\n",
    "# And that's it!  Now \"chain\" contains our samples!\n",
    "println(\"Chain:\", size(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can \"flatten\" that list of samples so that it's a (N x 2) array like for regular MCMC.\n",
    "flatchain, flat_llvals = AffineInvariantMCMC.flattenmcmcarray(chain, llvals);\n",
    "chain = flatchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "\n",
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-provision",
   "metadata": {},
   "source": [
    "With 10 walkers we got suprisingly good convergence and quickly. Lets ramp up the number of walkers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the sampler a number of things:\n",
    "# - how many dimensions are being sampled\n",
    "numdims = 2\n",
    "# - how many walkers we want\n",
    "numwalkers = 100\n",
    "# In the example code below, it runs an initial \"burn-in\" round of sampling, then does the \"real\" sampling.  This is how many\n",
    "# burn-in samples to take.\n",
    "nburn_aff = nburn Ã· numwalkers\n",
    "# - how many steps to take (after burn-in)\n",
    "nsteps_aff = (nsteps - nburn) Ã· numwalkers\n",
    "\n",
    "# We also need to give the walkers some initial positions.  NOTE that you can't give them all the same position!\n",
    "# Here, we're just drawing uniform numbers of between 0 and 1.\n",
    "x0 = rand(numdims, numwalkers)\n",
    "### FILL ME IN -- maybe you want to initialize the walkers around some better point?\n",
    "# x0[:,1] += ...?\n",
    "# x0[:,2] += ...?\n",
    "\n",
    "# We need to pass to the sampler a function that takes *only* the vector of parameters.  Our log_posterior_one function\n",
    "# also needs the (x,y,yerr) values.  So to make this work, we need a \"wrapper\" function to can grab the (x,y,yerr) values\n",
    "# and pass them to log_likelihood_one.\n",
    "\n",
    "ll_func(bm) = log_posterior_one(bm, (x, y, yerr))\n",
    "\n",
    "# Here we go, let's call the AffineInvariant MCMC's \"sample\" function -- this is the burn-in round!\n",
    "chain, _ = AffineInvariantMCMC.sample(ll_func, numwalkers, x0, nburn_aff, 1)\n",
    "\n",
    "# And here's the \"real\" run.\n",
    "# Notice that it's passing in the end of the burn-in chain as the initial position!\n",
    "chain, llvals = AffineInvariantMCMC.sample(ll_func, numwalkers, chain[:, :, end], nsteps_aff, 1)\n",
    "\n",
    "# And that's it!  Now \"chain\" contains our samples!\n",
    "println(\"Chain:\", size(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can \"flatten\" that list of samples so that it's a (N x 2) array like for regular MCMC.\n",
    "flatchain, flat_llvals = AffineInvariantMCMC.flattenmcmcarray(chain, llvals);\n",
    "chain = flatchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "\n",
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-prevention",
   "metadata": {},
   "source": [
    "Oof Thats not too good. Huge standard deviation and does not converge well. Maybe we just don't have enough walkers? 1000 WALKERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tell the sampler a number of things:\n",
    "# - how many dimensions are being sampled\n",
    "numdims = 2\n",
    "# - how many walkers we want\n",
    "numwalkers = 1000\n",
    "# In the example code below, it runs an initial \"burn-in\" round of sampling, then does the \"real\" sampling.  This is how many\n",
    "# burn-in samples to take.\n",
    "nburn_aff = nburn Ã· numwalkers\n",
    "# - how many steps to take (after burn-in)\n",
    "nsteps_aff = (nsteps - nburn) Ã· numwalkers\n",
    "\n",
    "# We also need to give the walkers some initial positions.  NOTE that you can't give them all the same position!\n",
    "# Here, we're just drawing uniform numbers of between 0 and 1.\n",
    "x0 = rand(numdims, numwalkers)\n",
    "### FILL ME IN -- maybe you want to initialize the walkers around some better point?\n",
    "# x0[:,1] += ...?\n",
    "# x0[:,2] += ...?\n",
    "\n",
    "# We need to pass to the sampler a function that takes *only* the vector of parameters.  Our log_posterior_one function\n",
    "# also needs the (x,y,yerr) values.  So to make this work, we need a \"wrapper\" function to can grab the (x,y,yerr) values\n",
    "# and pass them to log_likelihood_one.\n",
    "\n",
    "ll_func(bm) = log_posterior_one(bm, (x, y, yerr))\n",
    "\n",
    "# Here we go, let's call the AffineInvariant MCMC's \"sample\" function -- this is the burn-in round!\n",
    "chain, _ = AffineInvariantMCMC.sample(ll_func, numwalkers, x0, nburn_aff, 1)\n",
    "\n",
    "# And here's the \"real\" run.\n",
    "# Notice that it's passing in the end of the burn-in chain as the initial position!\n",
    "chain, llvals = AffineInvariantMCMC.sample(ll_func, numwalkers, chain[:, :, end], nsteps_aff, 1)\n",
    "\n",
    "# And that's it!  Now \"chain\" contains our samples!\n",
    "println(\"Chain:\", size(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can \"flatten\" that list of samples so that it's a (N x 2) array like for regular MCMC.\n",
    "flatchain, flat_llvals = AffineInvariantMCMC.flattenmcmcarray(chain, llvals);\n",
    "chain = flatchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "ax = Axis(f[1, 1], xlabel=\"MCMC Step\", ylabel=\"B\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,2], color=(:grey, 0.2), label=\"Samples\")\n",
    "\n",
    "# Let's measure the running mean and variance after different chain lengths\n",
    "runsteps = 500\n",
    "ss,mn,st = running_mean_std(chain[:,2], runsteps)\n",
    "\n",
    "scatter!(ss, mn, color=:black, label=\"Mean + Std up to this sample\")\n",
    "errorbars!(ss, mn, st)\n",
    "#Legend(f[1,2],ax)\n",
    "\n",
    "for i in 1:length(mn)\n",
    "    @printf(\"%.2f +/- %.2f \\n\", mn[i], st[i])\n",
    "end\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-developer",
   "metadata": {},
   "source": [
    "Still very bad. I think we just need more walkers. 10000 WALKERS!!!!! Just kidding the code breaks down with that many walkers. It looks like we got the best convergence with 10 walkers and the convergence got worse the more walkers we added. Just goes to show maybe more walkers is not the answer to everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-bread",
   "metadata": {},
   "source": [
    "The End!  Good work!\n",
    "\n",
    "Thank you! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-webcam",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
